{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "# POS_TAG is used to tag words in a sentence as nouns, verbs, adjectives, etc. This would help in determine sentence syntactic structure\n",
    "# WORD_TOKENIZE is used to split a sentence into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/prisha/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "# divide the text into a list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/prisha/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "œ# download the POS tagger for English language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Context Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric_heavy(text):\n",
    "    # Count the percentage of numeric content in the text , if more than 80% we don't consider it as a valid context\n",
    "    tokens = text.split()\n",
    "    numeric_tokens = [token for token in tokens if token.isdigit()]\n",
    "    return len(numeric_tokens) / len(tokens) > 0.8\n",
    "\n",
    "œœ\n",
    "def is_too_short(text):\n",
    "    # Define a minimum length for valid context atelast 10 words\n",
    "    return len(text.split()) < 10 \n",
    "\n",
    "def lacks_verbs(text):\n",
    "    # Use POS tagging to check if there are no verbs in the context,then also the context is not valid\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    verbs = [word for word, pos in tagged if pos.startswith('VB')]\n",
    "    return len(verbs) == 0 \n",
    "\n",
    "def is_repetitive(text):\n",
    "    # Check if the context contains a significant amount of repeated words, random context hence invalid\n",
    "    tokens = text.split()\n",
    "    unique_tokens = set(tokens)\n",
    "    return len(unique_tokens) / len(tokens) < 0.5  \n",
    "\n",
    "def is_valid_context(context):\n",
    "    if is_numeric_heavy(context) or is_too_short(context) or lacks_verbs(context) or is_repetitive(context):\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Question Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_question_generic(question):\n",
    "    # Detect generic or vague questions that mention about the context\n",
    "    generic_phrases = [\"What is mentioned\", \"from this context\", \"according to the context\" , \"in the given\"]\n",
    "    for phrase in generic_phrases:\n",
    "        if phrase in question.lower():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def question_mismatch_with_context(question, context):\n",
    "    # Check for keywords overlap between question and context , should be atleast 2 else the question is generic\n",
    "    question_keywords = set(re.findall(r'\\b\\w+\\b', question.lower()))\n",
    "    context_keywords = set(re.findall(r'\\b\\w+\\b', context.lower()))\n",
    "    common_words = question_keywords & context_keywords\n",
    "    return len(common_words) < 2 \n",
    "\n",
    "def is_question_incomplete(question):\n",
    "    # Detect incomplete questions\n",
    "    return question.endswith('...')\n",
    "\n",
    "def is_question_only_numbers(question):\n",
    "    # Detect questions that are only asking about numbers\n",
    "    return re.fullmatch(r'[\\d\\s]+', question)\n",
    "\n",
    "def is_valid_question(question, context):\n",
    "    # Combine criteria for determining if a question is invalid\n",
    "    if is_question_generic(question) or question_mismatch_with_context(question, context) or is_question_incomplete(question) or is_question_only_numbers(question):\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_json(input_file, output_file):\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    filtered_data = []\n",
    "\n",
    "    for doc in data['data']:\n",
    "        valid_paragraphs = []\n",
    "        for paragraph in doc['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            if is_valid_context(context):\n",
    "                valid_qas = []\n",
    "                for qa in paragraph['qas']:\n",
    "                    question = qa['question']\n",
    "                    if is_valid_question(question, context):\n",
    "                        valid_qas.append(qa)\n",
    "                \n",
    "                # Keep the paragraph only if there are valid questions\n",
    "                if valid_qas:\n",
    "                    paragraph['qas'] = valid_qas\n",
    "                    valid_paragraphs.append(paragraph)\n",
    "\n",
    "        # Keep the document only if it has valid paragraphs\n",
    "        if valid_paragraphs:\n",
    "            doc['paragraphs'] = valid_paragraphs\n",
    "            filtered_data.append(doc)\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump({'data': filtered_data}, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_json_2(input_file, output_file_valid, output_file_invalid):\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    filtered_data = []\n",
    "    removed_data = [] \n",
    "\n",
    "    for doc in data['data']:\n",
    "        valid_paragraphs = []\n",
    "        removed_paragraphs = []\n",
    "        for paragraph in doc['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            if is_valid_context(context):\n",
    "                valid_qas = []\n",
    "                invalid_qas = []\n",
    "                for qa in paragraph['qas']:\n",
    "                    question = qa['question']\n",
    "                    if is_valid_question(question, context):\n",
    "                        valid_qas.append(qa)\n",
    "                    else:\n",
    "                        invalid_qas.append(qa)\n",
    "\n",
    "                if valid_qas:\n",
    "                    paragraph['qas'] = valid_qas\n",
    "                    valid_paragraphs.append(paragraph)\n",
    "\n",
    "                if invalid_qas:\n",
    "                    invalid_paragraph = paragraph.copy()\n",
    "                    invalid_paragraph['qas'] = invalid_qas\n",
    "                    removed_paragraphs.append(invalid_paragraph)\n",
    "\n",
    "            else:\n",
    "                removed_paragraphs.append(paragraph)\n",
    "\n",
    "        if valid_paragraphs:\n",
    "            doc['paragraphs'] = valid_paragraphs\n",
    "            filtered_data.append(doc)\n",
    "\n",
    "        if removed_paragraphs:\n",
    "            doc_copy = doc.copy()\n",
    "            doc_copy['paragraphs'] = removed_paragraphs\n",
    "            removed_data.append(doc_copy)\n",
    "\n",
    "    # Save valid data\n",
    "    with open(output_file_valid, 'w') as f_valid:\n",
    "        json.dump({'data': filtered_data}, f_valid, indent=2)\n",
    "\n",
    "    # Save invalid data\n",
    "    with open(output_file_invalid, 'w') as f_invalid:\n",
    "        json.dump({'data': removed_data}, f_invalid, indent=2)\n",
    "\n",
    "    print(f\"Valid data saved to {output_file_valid}\")\n",
    "    print(f\"Invalid data saved to {output_file_invalid}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid data saved to filtered_output.json\n",
      "Invalid data saved to filtered_output_invalid.json\n"
     ]
    }
   ],
   "source": [
    "# Use the function to filter the input JSON\n",
    "input_file = 'dev.json'  \n",
    "output_file_valid = 'filtered_output.json' \n",
    "output_file_invalid = 'filtered_output_invalid.json'\n",
    "filter_json_2(input_file, output_file_valid, output_file_invalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function to filter the input JSON\n",
    "input_file = 'dev.json'  \n",
    "output_file = 'filtered_output.json' \n",
    "filter_json(input_file, output_file)\n",
    "print(\"Filtering completed. Valid data saved to filtered_output.json.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
