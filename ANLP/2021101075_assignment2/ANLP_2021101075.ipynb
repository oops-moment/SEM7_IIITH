{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZTOCuG7uzA4",
        "outputId": "1ba72624-d7aa-4c0a-bc4d-dd98dda1692a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.5)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=af78a3e1601f547d1fd445d313ec5606e24529937ce6185d641117c021d16020\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4UI6s8d7YM9K"
      },
      "outputs": [],
      "source": [
        "#@title IMPORT REQUIRED LIIBRARIES\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import re\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "import csv\n",
        "import os\n",
        "from google.colab import files\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A1lXBbpmcUk"
      },
      "source": [
        "## TESTING HELPER FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKzR3sbpW274",
        "outputId": "44e5779a-d46e-4d76-f4bd-528a3a472168"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU score for the first hypothesis (with smoothing): 0.19053627645285995\n"
          ]
        }
      ],
      "source": [
        "# Define references and hypotheses\n",
        "references_1 = [['this', 'is', 'a', 'test']]  # Reference for the first hypothesis\n",
        "hypothesis_1 = ['this', 'is', 'test']  # Hypothesis to evaluate\n",
        "\n",
        "# Define a smoothing function\n",
        "chencherry = SmoothingFunction()\n",
        "\n",
        "# Calculate BLEU score for the first sentence with smoothing\n",
        "score_1 = sentence_bleu(references_1, hypothesis_1, smoothing_function=chencherry.method1)\n",
        "print(f\"BLEU score for the first hypothesis (with smoothing): {score_1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80OyQlkvFmlm",
        "outputId": "f6c71d8d-febd-4896-a85c-e34c543f9f55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU score for the first hypothesis: 8.987727354491445e-155\n",
            "BLEU score for the second hypothesis: 1.5319719891192393e-231\n",
            "Score 1  {8.987727354491445e-155}\n",
            "Score 2  {1.5319719891192393e-231}\n",
            "Average BLEU score: 4.4938636772457226e-155\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ],
      "source": [
        "# Define references and hypotheses\n",
        "references_1 = [['this', 'is', 'a', 'test']]  # Reference for the first hypothesis\n",
        "hypothesis_1 = ['this', 'is', 'test']  # Hypothesis to evaluate\n",
        "\n",
        "# Calculate BLEU score for the first sentence\n",
        "score_1 = sentence_bleu(references_1, hypothesis_1)\n",
        "print(f\"BLEU score for the first hypothesis: {score_1}\")\n",
        "\n",
        "# Define second reference and hypothesis\n",
        "references_2 = [['another', 'sentence']]  # Reference for the second hypothesis\n",
        "hypothesis_2 = ['another', 'example']  # Hypothesis to evaluate\n",
        "\n",
        "# Calculate BLEU score for the second sentence\n",
        "score_2 = sentence_bleu(references_2, hypothesis_2)\n",
        "print(f\"BLEU score for the second hypothesis: {score_2}\")\n",
        "\n",
        "# Average BLEU score for both hypotheses\n",
        "average_score = (score_1 + score_2) / 2\n",
        "print(f\"Score 1 \",{score_1})\n",
        "print(f\"Score 2 \",{score_2})\n",
        "print(f\"Average BLEU score: {average_score}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAaFso2W53Or",
        "outputId": "571a9385-8855-4d79-de2b-61b212666631"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU score for the first hypothesis: 0.6237767360393223\n",
            "BLEU score for the second hypothesis: 1.6929529548569885e-62\n",
            "Score 1: 0.6237767360393223\n",
            "Score 2: 1.6929529548569885e-62\n",
            "Average BLEU score: 0.31188836801966113\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Define references and hypotheses\n",
        "references_1 = [['this', 'is', 'a', 'test']]  # Reference for the first hypothesis\n",
        "hypothesis_1 = ['this', 'is', 'test']  # Hypothesis to evaluate\n",
        "\n",
        "# Define weights for 1-gram, 2-gram, 3-gram, and 4-gram\n",
        "weights = (0.8, 0.2, 0, 0)\n",
        "\n",
        "# Calculate BLEU score for the first sentence with the defined weights\n",
        "score_1 = sentence_bleu(references_1, hypothesis_1, weights=weights)\n",
        "print(f\"BLEU score for the first hypothesis: {score_1}\")\n",
        "\n",
        "# Define second reference and hypothesis\n",
        "references_2 = [['another', 'sentence']]  # Reference for the second hypothesis\n",
        "hypothesis_2 = ['another', 'example']  # Hypothesis to evaluate\n",
        "\n",
        "# Calculate BLEU score for the second sentence with the defined weights\n",
        "score_2 = sentence_bleu(references_2, hypothesis_2, weights=weights)\n",
        "print(f\"BLEU score for the second hypothesis: {score_2}\")\n",
        "\n",
        "# Average BLEU score for both hypotheses\n",
        "average_score = (score_1 + score_2) / 2\n",
        "print(f\"Score 1: {score_1}\")\n",
        "print(f\"Score 2: {score_2}\")\n",
        "print(f\"Average BLEU score: {average_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QupewB-SzCVm"
      },
      "source": [
        "## TRANSFORMER ARCHITECTURE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ATDtOydNZif7"
      },
      "outputs": [],
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        #Scaling Input Embeddings by the factor sqrt(d_model)\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dtEyyMV0bo3G"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Create a matrix of shape (seq_len, d_model)\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "        # Create a vector of shape (seq_len)\n",
        "        position_value = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
        "        # Create a vector of shape (d_model)\n",
        "        divide_by = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
        "        pe[:, 0::2] = torch.sin(position_value * divide_by) # sin(position * (10000 ** (2i / d_model))\n",
        "        pe[:, 1::2] = torch.cos(position_value * divide_by) # cos(position * (10000 ** (2i / d_model))\n",
        "        pe = pe.unsqueeze(0) # (1, seq_len, d_model) , including batch\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
        "        x = self.dropout(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "juTE_rsjwa7l"
      },
      "outputs": [],
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, features: int, eps:float=10**-6) -> None:\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.alpha = nn.Parameter(torch.ones(features)) # multiply\n",
        "        self.bias = nn.Parameter(torch.zeros(features)) # addition\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
        "        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
        "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Rt4K6vnp1H94"
      },
      "outputs": [],
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch, seq_len, d_model->d_ff->d_model)\n",
        "        x= self.linear_1(x)\n",
        "        x = self.dropout(torch.relu(x))\n",
        "        x = self.linear_2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oQl35Ov41MUi"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.h = h\n",
        "        assert d_model % h == 0, \"Embeddings can't be distributed in h segments.\"\n",
        "\n",
        "        self.d_k = d_model // h   # vector size seen by h head\n",
        "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
        "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
        "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
        "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "        d_k = query.shape[-1]\n",
        "        # Just apply the formula from the paper\n",
        "        # (batch, h, seq_len, d_model)*(d_model,seq_len) --> (batch, h, seq_len, seq_len) QKT/root(dk)\n",
        "\n",
        "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "\n",
        "        if mask is not None:\n",
        "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
        "\n",
        "        if dropout is not None:\n",
        "            attention_scores = dropout(attention_scores)\n",
        "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)n\n",
        "        return (attention_scores @ value), attention_scores\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
        "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Calculate attention\n",
        "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "        # Combine all the heads together\n",
        "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
        "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
        "\n",
        "        # Multiply by Wo\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        return self.w_o(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7Pd7Ub4Q9ti1"
      },
      "outputs": [],
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "\n",
        "        def __init__(self, features: int, dropout: float) -> None:\n",
        "            super().__init__()\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "            self.norm = LayerNormalization(features)\n",
        "\n",
        "        def forward(self, x, sublayer):\n",
        "            # sublayer is the layer with whom you wan't to make residual connection with\n",
        "            return x + self.dropout(sublayer(self.norm(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ha7uv3HrPdjj"
      },
      "outputs": [],
      "source": [
        "class ProjectionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, vocab_size) -> None:\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x) -> None:\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
        "        return self.proj(x)   #here in the video it is return torch.log_softmax(self.proj(x),dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l15gHYxVHJhb"
      },
      "source": [
        "**ENCODER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HreglaUVFLbK"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(features,dropout),ResidualConnection(features,dropout)])\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
        "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OGmfzlMHADh"
      },
      "source": [
        "**DECODER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "C6NGJ6zDGxzm"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(features,dropout),ResidualConnection(features,dropout),ResidualConnection(features,dropout)])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
        "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "        x= self.norm(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7hQRnhMQ_7d"
      },
      "source": [
        "**TRANSFORMER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YF8VaO-1Qmbb"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.src_pos = src_pos\n",
        "        self.tgt_pos = tgt_pos\n",
        "        self.projection_layer = projection_layer\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        # (batch, seq_len, d_model)\n",
        "        src = self.src_embed(src)\n",
        "        src = self.src_pos(src)\n",
        "        return self.encoder(src, src_mask)\n",
        "\n",
        "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
        "        # (batch, seq_len, d_model)\n",
        "        tgt = self.tgt_embed(tgt)\n",
        "        tgt = self.tgt_pos(tgt)\n",
        "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "    def project(self, x):\n",
        "        # (batch, seq_len, vocab_size)\n",
        "        return self.projection_layer(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zam42RquSxrS"
      },
      "outputs": [],
      "source": [
        "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model, N, h, dropout, d_ff) -> Transformer:\n",
        "    # Create the embedding layers\n",
        "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
        "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Create the positional encoding layers\n",
        "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
        "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
        "\n",
        "    # Create the encoder blocks\n",
        "    encoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
        "        encoder_blocks.append(encoder_block)\n",
        "\n",
        "    # Create the decoder blocks\n",
        "    decoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
        "        decoder_blocks.append(decoder_block)\n",
        "\n",
        "    # Create the encoder and decoder\n",
        "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
        "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
        "\n",
        "    # Create the projection layer\n",
        "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Create the transformer\n",
        "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
        "\n",
        "    # Initialize the parameters\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKXIUovlu0yB"
      },
      "source": [
        "**LOADING DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "tdlnZIbWz1Pu"
      },
      "outputs": [],
      "source": [
        "def read_datasets(inputfilename,outputfilename):\n",
        "    ds = {}\n",
        "    with open(inputfilename, 'r', encoding='utf-8') as file_en:\n",
        "        ds['english'] = [line.strip() for line in file_en.readlines()]\n",
        "    with open(outputfilename, 'r', encoding='utf-8') as file_fr:\n",
        "        ds['french'] = [line.strip() for line in file_fr.readlines()]\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ozsrmD8k3Rc0"
      },
      "outputs": [],
      "source": [
        "def calculate_statistics(ds, lang):\n",
        "    if lang == 'en':\n",
        "        sentences = ds['english']\n",
        "    elif lang == 'fr':\n",
        "        sentences = ds['french']\n",
        "\n",
        "    lengths = [len(sentence.split()) for sentence in sentences]\n",
        "\n",
        "    average_length = sum(lengths) / len(lengths) if lengths else 0\n",
        "    min_length = min(lengths) if lengths else 0\n",
        "    max_length = max(lengths) if lengths else 0\n",
        "\n",
        "    return average_length, min_length, max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "GGsIc0xihjGr"
      },
      "outputs": [],
      "source": [
        "def truncate_sentences(ds, lang, max_tokens=55):\n",
        "    if lang == 'en':\n",
        "        lines = ds['english']\n",
        "    elif lang == 'fr':\n",
        "        lines = ds['french']\n",
        "\n",
        "    # Update the sentences in the dataset\n",
        "    for i in range(len(lines)):\n",
        "        tokenized_line = lines[i].split()\n",
        "        if len(tokenized_line) > max_tokens:\n",
        "            tokenized_line = tokenized_line[:max_tokens]\n",
        "        lines[i] = ' '.join(tokenized_line).strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Na2SL1fqzH4R"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"Remove special characters from the text.\"\"\"\n",
        "    cleaned_text = re.sub(r'[^a-zA-Z0-9éèêëôîâä\\s]', '', text)\n",
        "    return cleaned_text.strip()\n",
        "\n",
        "\n",
        "def clean_dataset(ds):\n",
        "    \"\"\"Remove special characters from sentences in the dataset.\"\"\"\n",
        "    ds['english'] = [clean_text(sentence) for sentence in ds['english']]\n",
        "    ds['french'] = [clean_text(sentence) for sentence in ds['french']]\n",
        "    return ds\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-d4RBlyC2EZA"
      },
      "outputs": [],
      "source": [
        "train_input_sentences_file='train.en'\n",
        "train_output_sentences_file='train.fr'\n",
        "\n",
        "val_input_sentences_file='dev.en'\n",
        "val_output_sentences_file='dev.fr'\n",
        "\n",
        "test_input_sentences_file='test.en'\n",
        "test_output_sentences_file='test.fr'\n",
        "\n",
        "\n",
        "train_ds=read_datasets(train_input_sentences_file,train_output_sentences_file)\n",
        "val_ds=read_datasets(val_input_sentences_file,val_output_sentences_file)\n",
        "test_ds=read_datasets(test_input_sentences_file,test_output_sentences_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMp8oJvrIwun",
        "outputId": "8a718945-6a15-43a8-e406-57d589f72a36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30000\n",
            "30000\n"
          ]
        }
      ],
      "source": [
        "print(len(train_ds['english']))\n",
        "print(len(train_ds['french']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSOVORKEJB0h",
        "outputId": "e7c17a8b-325a-446b-a57c-33860b7b0eb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30000\n",
            "30000\n"
          ]
        }
      ],
      "source": [
        "train_ds = clean_dataset(train_ds)\n",
        "val_ds = clean_dataset(val_ds)\n",
        "test_ds = clean_dataset(test_ds)\n",
        "\n",
        "print(len(train_ds['english']))\n",
        "print(len(train_ds['french']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "1Qa9igEiwDNg"
      },
      "outputs": [],
      "source": [
        "def get_or_build_tokenizer( ds, lang):\n",
        "    tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "    tokenizer.pre_tokenizer = Whitespace()\n",
        "    trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
        "    tokenizer.train_from_iterator(get_all_sentences(ds, lang,55), trainer=trainer)\n",
        "    return tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "oUiwZsFoz70T"
      },
      "outputs": [],
      "source": [
        "def get_all_sentences(ds, lang, max_tokens=55):\n",
        "    max_len = 0\n",
        "    if lang == 'en':\n",
        "        lines = ds['english']\n",
        "    elif lang == 'fr':\n",
        "        lines = ds['french']\n",
        "\n",
        "    for line in lines:\n",
        "        tokenized_line = line.split()\n",
        "        if len(tokenized_line) > max_tokens:\n",
        "            tokenized_line = tokenized_line[:max_tokens]\n",
        "        max_len = max(max_len, len(tokenized_line))\n",
        "        yield ' '.join(tokenized_line).strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "UguZ9UYO05pl"
      },
      "outputs": [],
      "source": [
        "tokenizer_src=get_or_build_tokenizer(train_ds, 'en')\n",
        "tokenizer_tgt=get_or_build_tokenizer(train_ds, 'fr')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0-FTJGOGipP",
        "outputId": "af695312-7ba7-4a43-b746-f0a8c2f62427"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14249\n",
            "17983\n"
          ]
        }
      ],
      "source": [
        "vocab_size_src=tokenizer_src.get_vocab_size()\n",
        "vocab_size_tgt=tokenizer_tgt.get_vocab_size()\n",
        "print(vocab_size_src)\n",
        "print(vocab_size_tgt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWVhFt7C1oWb",
        "outputId": "34d810f8-2ff4-45b1-ecbd-353ece2c5feb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average length of English sentences: 16.857066666666668\n",
            "Minimum length of English sentences: 1\n",
            "Maximum length of English sentences: 498\n",
            "Average length of French sentences: 17.453766666666667\n",
            "Minimum length of French sentences: 0\n",
            "Maximum length of French sentences: 490\n"
          ]
        }
      ],
      "source": [
        "average_length_english, min_length_english, max_length_english = calculate_statistics(train_ds, 'en')\n",
        "average_length_french, min_length_french, max_length_french = calculate_statistics(train_ds, 'fr')\n",
        "\n",
        "print(f\"Average length of English sentences: {average_length_english}\")\n",
        "print(f\"Minimum length of English sentences: {min_length_english}\")\n",
        "print(f\"Maximum length of English sentences: {max_length_english}\")\n",
        "\n",
        "print(f\"Average length of French sentences: {average_length_french}\")\n",
        "print(f\"Minimum length of French sentences: {min_length_french}\")\n",
        "print(f\"Maximum length of French sentences: {max_length_french}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziHwj0XmVGOq",
        "outputId": "fb2caab3-f4f7-4ce1-993d-7b76ca14d3af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30000\n",
            "30000\n"
          ]
        }
      ],
      "source": [
        "truncate_sentences(train_ds, 'en',55)\n",
        "truncate_sentences(train_ds, 'fr',55)\n",
        "\n",
        "truncate_sentences(val_ds, 'en',55)\n",
        "truncate_sentences(val_ds, 'fr',55)\n",
        "\n",
        "truncate_sentences(test_ds, 'en',55)\n",
        "truncate_sentences(test_ds, 'fr',55)\n",
        "\n",
        "print(len(train_ds['english']))\n",
        "print(len(train_ds['french']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7VEyme9oCzd",
        "outputId": "70196a50-68cf-4ec1-de10-2f61cad70b15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55\n",
            "55\n",
            "55\n"
          ]
        }
      ],
      "source": [
        "maximum_length_src = 0\n",
        "maximum_length_tgt = 0\n",
        "\n",
        "for en_sentence, fr_sentence in zip(val_ds['english'], val_ds['french']):\n",
        "    src_ids = tokenizer_src.encode(en_sentence).ids\n",
        "    tgt_ids = tokenizer_tgt.encode(fr_sentence).ids\n",
        "    maximum_length_src = max(maximum_length_src, len(src_ids))\n",
        "    maximum_length_tgt = max(maximum_length_tgt, len(tgt_ids))\n",
        "\n",
        "print(maximum_length_src)\n",
        "print(maximum_length_tgt)\n",
        "\n",
        "maximum_sequence_length=max(maximum_length_src,maximum_length_tgt)\n",
        "print(maximum_sequence_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FJJsDwj3GlH"
      },
      "source": [
        "**CREATING DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "lrBgItBz3CSd"
      },
      "outputs": [],
      "source": [
        "class BuildDataset(Dataset):\n",
        "\n",
        "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.ds = ds\n",
        "        self.tokenizer_src = tokenizer_src\n",
        "        self.tokenizer_tgt = tokenizer_tgt\n",
        "        self.src_lang = src_lang\n",
        "        self.tgt_lang = tgt_lang\n",
        "        tokens=[torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64),torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64),torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)]\n",
        "        self.sos_token= tokens[0]\n",
        "        self.eos_token= tokens[1]\n",
        "        self.pad_token= tokens[2]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds[self.src_lang])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_text = self.ds[self.src_lang][idx]\n",
        "        tgt_text = self.ds[self.tgt_lang][idx]\n",
        "\n",
        "\n",
        "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
        "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
        "\n",
        "        # Calculate the required padding\n",
        "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # For <sos> and <eos>\n",
        "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1  # For <sos>\n",
        "\n",
        "\n",
        "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
        "            raise ValueError(\"Sentence is too long\")\n",
        "\n",
        "        # Create encoder input\n",
        "        encoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # Create decoder input\n",
        "        decoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # Create label (with eos token)\n",
        "        label = torch.cat(\n",
        "            [\n",
        "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # Ensure the sizes are correct\n",
        "\n",
        "        if(encoder_input.size(0)!=self.seq_len or decoder_input.size(0)!=self.seq_len or label.size(0)!=self.seq_len):\n",
        "            raise ValueError(\"Length could not be made equal to sequence length\")\n",
        "\n",
        "        return {\n",
        "            \"encoder_input\": encoder_input,\n",
        "            \"decoder_input\": decoder_input,\n",
        "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),  # (1, 1, seq_len)\n",
        "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),  # (1, seq_len) & (1, seq_len, seq_len)\n",
        "            \"label\": label,\n",
        "            \"src_text\": src_text,\n",
        "            \"tgt_text\": tgt_text,\n",
        "        }\n",
        "\n",
        "def causal_mask(size):\n",
        "    mask = torch.ones((1, size, size))\n",
        "    mask = mask.triu(diagonal=1).type(torch.int)\n",
        "    return mask == 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "OxjZzHAE7MFb"
      },
      "outputs": [],
      "source": [
        "training_dataset = BuildDataset(train_ds, tokenizer_src, tokenizer_tgt, 'english', 'french', seq_len=65)\n",
        "val_dataset = BuildDataset(val_ds, tokenizer_src, tokenizer_tgt, 'english', 'french', seq_len=65)\n",
        "test_dataset= BuildDataset(test_ds, tokenizer_src, tokenizer_tgt, 'english', 'french', seq_len=65)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQ4DSDGlITpX",
        "outputId": "c985c293-bdf7-4954-ece1-467f7c5002c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30000\n",
            "30000\n",
            "30000\n"
          ]
        }
      ],
      "source": [
        "print(len(training_dataset))\n",
        "print(len(train_ds['english']))\n",
        "print(len(train_ds['french']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "G4wL2Eckoq0Q"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'batch_size': 32,\n",
        "    'src_seq_len':65,\n",
        "    'tgt_seq_len': 65,\n",
        "    'vocab_src': tokenizer_src.get_vocab_size() ,\n",
        "    'vocab_tgt': tokenizer_tgt.get_vocab_size(),\n",
        "    'd_model': 300 ,\n",
        "    'N': 2,\n",
        "    'h': 4,\n",
        "    'dropout': 0.1,\n",
        "    'd_ff': 1024,\n",
        "    'learning_rate': 0.0005,\n",
        "    'num_epochs': 15,\n",
        "    'eps':1e-9\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "ge62KA7em74p"
      },
      "outputs": [],
      "source": [
        "train_dataloader= DataLoader(training_dataset,config['batch_size'], shuffle=True)\n",
        "val_dataloader= DataLoader(val_dataset,config['batch_size'], shuffle=True)\n",
        "test_dataloader= DataLoader(test_dataset,1, shuffle=True) # since we need sentence wise bleu score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "TPagqk5PoXCo"
      },
      "outputs": [],
      "source": [
        "model=build_transformer(config['vocab_src'], config['vocab_tgt'], config['src_seq_len'], config['tgt_seq_len'], config['d_model'], config['N'], config['h'], config['dropout'], config['d_ff'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g89yS0gepj6j",
        "outputId": "a001accf-7cca-4a1c-e3a8-74b3cff58ef4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8lP8h2hrpuB",
        "outputId": "32852085-6751-4492-ecc4-fba9338398da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-1): 2 x EncoderBlock(\n",
              "        (self_attention_block): MultiHeadAttentionBlock(\n",
              "          (w_q): Linear(in_features=300, out_features=300, bias=False)\n",
              "          (w_k): Linear(in_features=300, out_features=300, bias=False)\n",
              "          (w_v): Linear(in_features=300, out_features=300, bias=False)\n",
              "          (w_o): Linear(in_features=300, out_features=300, bias=False)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward_block): FeedForwardBlock(\n",
              "          (linear_1): Linear(in_features=300, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear_2): Linear(in_features=1024, out_features=300, bias=True)\n",
              "        )\n",
              "        (residual_connections): ModuleList(\n",
              "          (0-1): 2 x ResidualConnection(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (norm): LayerNormalization()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNormalization()\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-1): 2 x DecoderBlock(\n",
              "        (self_attention_block): MultiHeadAttentionBlock(\n",
              "          (w_q): Linear(in_features=300, out_features=300, bias=False)\n",
              "          (w_k): Linear(in_features=300, out_features=300, bias=False)\n",
              "          (w_v): Linear(in_features=300, out_features=300, bias=False)\n",
              "          (w_o): Linear(in_features=300, out_features=300, bias=False)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (cross_attention_block): MultiHeadAttentionBlock(\n",
              "          (w_q): Linear(in_features=300, out_features=300, bias=False)\n",
              "          (w_k): Linear(in_features=300, out_features=300, bias=False)\n",
              "          (w_v): Linear(in_features=300, out_features=300, bias=False)\n",
              "          (w_o): Linear(in_features=300, out_features=300, bias=False)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward_block): FeedForwardBlock(\n",
              "          (linear_1): Linear(in_features=300, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear_2): Linear(in_features=1024, out_features=300, bias=True)\n",
              "        )\n",
              "        (residual_connections): ModuleList(\n",
              "          (0-2): 3 x ResidualConnection(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (norm): LayerNormalization()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNormalization()\n",
              "  )\n",
              "  (src_embed): InputEmbeddings(\n",
              "    (embedding): Embedding(14249, 300)\n",
              "  )\n",
              "  (tgt_embed): InputEmbeddings(\n",
              "    (embedding): Embedding(17983, 300)\n",
              "  )\n",
              "  (src_pos): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (tgt_pos): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (projection_layer): ProjectionLayer(\n",
              "    (proj): Linear(in_features=300, out_features=17983, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "NtthNE1Vr3XX"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'], betas=(0.9, 0.98),eps=config['eps'])\n",
        "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.1, min_lr=1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "qVz5Uanxr946"
      },
      "outputs": [],
      "source": [
        "loss_fn=nn.CrossEntropyLoss(ignore_index=tokenizer_tgt.token_to_id('[PAD]')).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "Y365oDaI-V2G"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics(model, dataloader, tokenizer_tgt, device,loss_fn):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_references = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0\n",
        "        for batch in tqdm(dataloader, desc=\"Calculating Metrics\"):\n",
        "            encoder_input = batch['encoder_input'].to(device)\n",
        "            decoder_input = batch['decoder_input'].to(device)\n",
        "            encoder_mask = batch['encoder_mask'].to(device)\n",
        "            decoder_mask = batch['decoder_mask'].to(device)\n",
        "            label = batch['label'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            encoder_output = model.encode(encoder_input, encoder_mask)\n",
        "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
        "            proj_output = model.project(decoder_output)\n",
        "\n",
        "            # Get predictions (argmax)\n",
        "            predictions = torch.argmax(proj_output, dim=-1)\n",
        "\n",
        "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Convert to list of sentences (token IDs to words)\n",
        "            for i in range(predictions.size(0)):\n",
        "                pred_sentence = predictions[i].cpu().numpy()\n",
        "                ref_sentence = label[i].cpu().numpy()\n",
        "\n",
        "                # Remove padding (0s) from predictions and references\n",
        "                pred_sentence = pred_sentence[pred_sentence != 0]\n",
        "                ref_sentence = ref_sentence[ref_sentence != 0]\n",
        "\n",
        "                # Decode sentences using the tokenizer\n",
        "                pred_sentence = tokenizer_tgt.decode(pred_sentence)\n",
        "                ref_sentence = tokenizer_tgt.decode(ref_sentence)\n",
        "\n",
        "                all_predictions.append(pred_sentence)\n",
        "                all_references.append(ref_sentence)\n",
        "\n",
        "    # Compute BLEU score\n",
        "    references = [[ref.split()] for ref in all_references]  # List of references for BLEU\n",
        "    predictions = [pred.split() for pred in all_predictions]  # List of predictions for BLEU\n",
        "    bleu_score = corpus_bleu(references, predictions,smoothing_function=SmoothingFunction().method1)\n",
        "\n",
        "    # Compute ROUGE score\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    rouge_scores = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}\n",
        "    for ref, pred in zip(all_references, all_predictions):\n",
        "        scores = scorer.score(ref, pred)\n",
        "        for key in rouge_scores:\n",
        "            rouge_scores[key] += scores[key].fmeasure  # Accumulate scores\n",
        "\n",
        "    # Average ROUGE scores\n",
        "    num_samples = len(all_references)\n",
        "    for key in rouge_scores:\n",
        "        rouge_scores[key] /= num_samples\n",
        "\n",
        "    return bleu_score,rouge_scores, total_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "Lope8HfKuAt4"
      },
      "outputs": [],
      "source": [
        "csv_file = 'training_log.csv'\n",
        "\n",
        "# Function to initialize the CSV file and log the model configuration\n",
        "def init_csv_logging(config, csv_file):\n",
        "    if os.path.exists(csv_file):\n",
        "        os.remove(csv_file)\n",
        "\n",
        "    with open(csv_file, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Model Configuration'])\n",
        "        for key, value in config.items():\n",
        "            writer.writerow([key, value])\n",
        "        writer.writerow(['Epoch', 'Train Loss', 'Validation Loss', 'BLEU Score', 'ROUGE1', 'ROUGE2', 'ROUGE-L'])\n",
        "\n",
        "def log_metrics(epoch, train_loss, val_loss, bleu_score, rouge_scores, csv_file):\n",
        "    with open(csv_file, mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\n",
        "            epoch,\n",
        "            train_loss,\n",
        "            val_loss,\n",
        "            bleu_score,\n",
        "            rouge_scores['rouge1'],\n",
        "            rouge_scores['rouge2'],\n",
        "            rouge_scores['rougeL']\n",
        "        ])\n",
        "\n",
        "init_csv_logging(config, csv_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIC8ImwmsjN9",
        "outputId": "e837fd62-93d3-4456-cf62-506c4fc28d53"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 938/938 [01:00<00:00, 15.59it/s, loss=5.163]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Training Loss: 5.8926\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Metrics: 100%|██████████| 28/28 [00:00<00:00, 32.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Validation Loss: 5.0596, \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 938/938 [00:59<00:00, 15.65it/s, loss=4.398]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Training Loss: 4.7643\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Metrics: 100%|██████████| 28/28 [00:00<00:00, 31.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Validation Loss: 4.5304, \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 938/938 [00:59<00:00, 15.67it/s, loss=3.859]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/15], Training Loss: 4.0955\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Metrics: 100%|██████████| 28/28 [00:01<00:00, 24.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/15], Validation Loss: 4.1664, \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 938/938 [01:00<00:00, 15.62it/s, loss=3.939]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/15], Training Loss: 3.5794\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Metrics: 100%|██████████| 28/28 [00:00<00:00, 31.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/15], Validation Loss: 3.9363, \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|██████████| 938/938 [00:59<00:00, 15.67it/s, loss=3.499]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/15], Training Loss: 3.1815\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Metrics: 100%|██████████| 28/28 [00:00<00:00, 32.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/15], Validation Loss: 3.8548, \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|██████████| 938/938 [01:00<00:00, 15.60it/s, loss=3.245]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [6/15], Training Loss: 2.8702\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Metrics: 100%|██████████| 28/28 [00:00<00:00, 31.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [6/15], Validation Loss: 3.8354, \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|██████████| 938/938 [01:00<00:00, 15.61it/s, loss=2.684]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [7/15], Training Loss: 2.6234\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Metrics: 100%|██████████| 28/28 [00:00<00:00, 32.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [7/15], Validation Loss: 3.8191, \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|██████████| 938/938 [00:59<00:00, 15.65it/s, loss=2.572]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [8/15], Training Loss: 2.4232\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Metrics: 100%|██████████| 28/28 [00:01<00:00, 25.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [8/15], Validation Loss: 3.8173, \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|██████████| 938/938 [00:59<00:00, 15.66it/s, loss=2.652]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [9/15], Training Loss: 2.2547\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Metrics: 100%|██████████| 28/28 [00:01<00:00, 25.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [9/15], Validation Loss: 3.8758, \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|██████████| 938/938 [00:59<00:00, 15.64it/s, loss=2.833]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/15], Training Loss: 2.1079\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Metrics: 100%|██████████| 28/28 [00:00<00:00, 32.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/15], Validation Loss: 3.9398, \n",
            "Early stopping triggered.\n"
          ]
        }
      ],
      "source": [
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "patience = 2\n",
        "\n",
        "for epoch in range(config['num_epochs']):\n",
        "    model.train()\n",
        "    batch_iterator = tqdm(train_dataloader, desc=f'Epoch {epoch +1}')\n",
        "\n",
        "    total_loss = 0  # Initialize total loss for the epoch\n",
        "    for batch in batch_iterator:\n",
        "        encoder_input = batch['encoder_input'].to(device)  # (batchsize, seq_len)\n",
        "        decoder_input = batch['decoder_input'].to(device)  # (batchsize, seq_len)\n",
        "        encoder_mask = batch['encoder_mask'].to(device)    # (batchsize, 1, 1, seq_len)\n",
        "        decoder_mask = batch['decoder_mask'].to(device)    # (batchsize, 1, seq_len, seq_len)\n",
        "        label = batch['label'].to(device)                  # (batchsize, seq_len)\n",
        "\n",
        "        # Forward pass\n",
        "        encoder_output = model.encode(encoder_input, encoder_mask)  # (batchsize, seq_len, d_model)\n",
        "        decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)  # (batchsize, seq_len, d_model)\n",
        "        proj_output = model.project(decoder_output)  # (batchsize, seq_len, vocab_size)\n",
        "\n",
        "        loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        batch_iterator.set_postfix({'loss': f'{loss.item():6.3f}'})\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Update optimizer\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch [{epoch + 1}/{config['num_epochs']}], Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Validate the model\n",
        "    with torch.no_grad():\n",
        "        bleu_score,rouge_scores_val, val_loss = calculate_metrics(model, val_dataloader, tokenizer_tgt, device, loss_fn)\n",
        "\n",
        "    # scheduler.step(val_loss)\n",
        "\n",
        "    # Print the validation results\n",
        "    print(f\"Epoch [{epoch + 1}/{config['num_epochs']}], \"\n",
        "          f\"Validation Loss: {val_loss:.4f}, \")\n",
        "\n",
        "    # Log metrics to the CSV file (separate columns for rouge1, rouge2, and rougeL)\n",
        "    log_metrics(epoch + 1, avg_train_loss, val_loss, bleu_score, rouge_scores_val, csv_file)\n",
        "\n",
        "    # Check for early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), 'best_model.pt')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if patience_counter >= patience:\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnKxw2ev08jK",
        "outputId": "8d9fa187-12b2-4cf3-e853-27c9d1309ae9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-62-7f68d31eee5b>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pt'))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('best_model.pt'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "L-CmGH0pEsed"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics_print(model, dataloader, tokenizer_tgt, device, loss_fn, output_file='testbleu.txt'):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_references = []\n",
        "    num_samples = 0  # To keep track of the number of samples\n",
        "\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0\n",
        "        for batch in tqdm(dataloader, desc=\"Calculating Metrics\"):\n",
        "            encoder_input = batch['encoder_input'].to(device)\n",
        "            decoder_input = batch['decoder_input'].to(device)\n",
        "            encoder_mask = batch['encoder_mask'].to(device)\n",
        "            decoder_mask = batch['decoder_mask'].to(device)\n",
        "            label = batch['label'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            encoder_output = model.encode(encoder_input, encoder_mask)\n",
        "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
        "            proj_output = model.project(decoder_output)\n",
        "\n",
        "            # Get predictions (argmax)\n",
        "            predictions = torch.argmax(proj_output, dim=-1)\n",
        "\n",
        "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Convert to list of sentences (token IDs to words)\n",
        "            for i in range(predictions.size(0)):\n",
        "                pred_sentence = predictions[i].cpu().numpy()\n",
        "                ref_sentence = label[i].cpu().numpy()\n",
        "\n",
        "                # Remove padding (0s) from predictions and references\n",
        "                pred_sentence = pred_sentence[pred_sentence != 0]\n",
        "                ref_sentence = ref_sentence[ref_sentence != 0]\n",
        "\n",
        "                # Decode sentences using the tokenizer\n",
        "                pred_sentence_decoded = tokenizer_tgt.decode(pred_sentence)\n",
        "                ref_sentence_decoded = tokenizer_tgt.decode(ref_sentence)\n",
        "\n",
        "                all_predictions.append(pred_sentence_decoded)\n",
        "                all_references.append(ref_sentence_decoded)\n",
        "                num_samples += 1  # Increment sample count\n",
        "\n",
        "\n",
        "    # Open the output file for writing scores\n",
        "    total_bleu_score=0\n",
        "    with open(output_file, 'w') as f:  # Open the output file\n",
        "        # Initialize ROUGE scorer\n",
        "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "        # Compute BLEU and ROUGE scores for each sentence\n",
        "        for idx, (ref, pred) in enumerate(zip(all_references, all_predictions), start=1):\n",
        "            # Calculate BLEU score for the current prediction\n",
        "            reference_tokens = [ref.split()]  # BLEU expects a list of reference sentences\n",
        "            prediction_tokens = pred.split()\n",
        "            sentence_bleu_score = sentence_bleu(reference_tokens, prediction_tokens, smoothing_function=SmoothingFunction().method1)\n",
        "            total_bleu_score+=sentence_bleu_score\n",
        "\n",
        "            # Calculate ROUGE scores\n",
        "            scores = scorer.score(ref, pred)\n",
        "            rouge1_score = scores['rouge1'].fmeasure\n",
        "            rouge2_score = scores['rouge2'].fmeasure\n",
        "            rougeL_score = scores['rougeL'].fmeasure\n",
        "\n",
        "            # Write scores to the file\n",
        "            f.write(f\"{idx}\\t{sentence_bleu_score}\\t{rouge1_score:.4f}\\t{rouge2_score:.4f}\\t{rougeL_score:.4f}\\n\")\n",
        "\n",
        "\n",
        "    average_bleu_score = total_bleu_score / num_samples\n",
        "    rouge_scores = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}\n",
        "    for ref, pred in zip(all_references, all_predictions):\n",
        "        scores = scorer.score(ref, pred)\n",
        "        for key in rouge_scores:\n",
        "            rouge_scores[key] += scores[key].fmeasure  # Accumulate scores\n",
        "\n",
        "    # Average ROUGE scores\n",
        "    for key in rouge_scores:\n",
        "        rouge_scores[key] /= num_samples\n",
        "\n",
        "    # Append overall scores to the output file\n",
        "    with open(output_file, 'a') as f:  # Append to the same file\n",
        "        f.write(f\"\\nOverall Corpus BLEU Score: {average_bleu_score:.4f}\\n\")\n",
        "        f.write(f\"Average ROUGE1 Score: {rouge_scores['rouge1']:.4f}\\n\")\n",
        "        f.write(f\"Average ROUGE2 Score: {rouge_scores['rouge2']:.4f}\\n\")\n",
        "        f.write(f\"Average ROUGEL Score: {rouge_scores['rougeL']:.4f}\\n\")\n",
        "\n",
        "    return average_bleu_score, rouge_scores, total_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkLsFu8nE63Z",
        "outputId": "f711e7a9-18e1-40ca-903f-e35864e78505"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Metrics: 100%|██████████| 1305/1305 [00:12<00:00, 107.62it/s]\n"
          ]
        }
      ],
      "source": [
        "bleu_score, rouge_scores, test_loss = calculate_metrics_print(model, test_dataloader, tokenizer_tgt, device, loss_fn, output_file='testbleu.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2Wf64i9FIBr",
        "outputId": "660b61f4-af4b-4957-ec50-1fd3c9086e98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_bleu_score 0.16386123974542996\n",
            "test_rouge_scores {'rouge1': 0.3542003792159551, 'rouge2': 0.17437773694482706, 'rougeL': 0.3306821787291517}\n",
            "test_loss 3.1660777951861907\n"
          ]
        }
      ],
      "source": [
        "print(\"test_bleu_score\",bleu_score)\n",
        "print(\"test_rouge_scores\",rouge_scores)\n",
        "print(\"test_loss\",test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mx_uJtdATZUB",
        "outputId": "e8576b89-1907-4e9a-d54f-e56000bdad5c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Metrics: 100%|██████████| 938/938 [00:30<00:00, 30.98it/s]\n"
          ]
        }
      ],
      "source": [
        "train_bleu_score, train_rouge_scores, train_loss = calculate_metrics_print(model, train_dataloader, tokenizer_tgt, device, loss_fn, output_file='trainbleu.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ir_5bpStd7QW",
        "outputId": "78e11d08-e107-4601-ecb0-7c8e390050fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_bleu_score 0.26966030869829216\n",
            "train_rouge_scores {'rouge1': 0.4634462829679052, 'rouge2': 0.2943950243586729, 'rougeL': 0.45088743113187324}\n",
            "train_loss 1.7405201078477952\n"
          ]
        }
      ],
      "source": [
        "print(\"train_bleu_score\",train_bleu_score)\n",
        "print(\"train_rouge_scores\",train_rouge_scores)\n",
        "print(\"train_loss\",train_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "7HZCj_ARyjXo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
