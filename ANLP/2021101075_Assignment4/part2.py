# -*- coding: utf-8 -*-
"""PART2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NfXSDuVZJ_pz8oByPaTa3enbBAAS0IVh
"""

# pip install torch transformers datasets numpy matplotlib tqdm psutil bitsandbytes

# !pip install -U bitsandbytes

"""## Install Desired Libraries"""

import torch
import transformers
from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2LMHeadModel
from datasets import load_dataset
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import psutil
import os
import time
import gc
import bitsandbytes as bnb
from torch.utils.data import Dataset, DataLoader
import copy

"""## Create the text dataset"""

class TextDataset(Dataset):
    """Custom Dataset for GPT2"""
    def __init__(self, encodings):
        self.encodings = encodings

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        return item

    def __len__(self):
        return len(self.encodings.input_ids)

"""## Calculate Memory Requirement for the Model"""

def calculate_model_memory(model, quantized_bits=32):
    """
    Calculates the memory usage of the model. If quantized, it assumes
    that the model weights are quantized to the specified bit-width.

    Parameters:
    - model: The PyTorch model
    - quantized_bits: The bit-width of the quantized model weights. Can be 32, 8, or 4.

    Returns:
    - Memory usage in MB
    """
    total_params = sum(p.numel() for p in model.parameters())

    # Set the number of bytes per parameter based on the quantization bit-width
    if quantized_bits == 32:
        param_size_in_bytes = 4  # 4 bytes per parameter for 32-bit (default float)
    elif quantized_bits == 8:
        param_size_in_bytes = 1  # 1 byte per parameter for 8-bit quantization
    elif quantized_bits == 4:
        param_size_in_bytes = 0.5  # 0.5 byte per parameter for 4-bit quantization
    else:
        raise ValueError("Supported quantized bit-widths are 32, 8, and 4.")

    # Calculate total memory usage (in bytes), then convert to MB
    total_memory = total_params * param_size_in_bytes / (1024 * 1024)  # Convert bytes to MB
    return total_memory

"""## Create the Quantisation Class"""

class ModelQuantizer:
    """Handles different quantization methods using bitsandbytes"""
    def __init__(self, model_name='gpt2', device='cuda'):
        self.model_name = model_name
        self.device = device
        self.original_model = None
        self.model_8bit = None
        self.model_4bit = None
        self.model_nf4 = None

    def load_original_model(self):
        """Load the original FP16/32 model"""
        self.original_model = AutoModelForCausalLM.from_pretrained(
            self.model_name
        ).to(self.device)
        return self.original_model

    def load_8bit_model(self):
        """Load 8-bit quantized model"""
        self.model_8bit = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            load_in_8bit=True,
            device_map='auto'
        )
        torch.save(self.model_8bit.state_dict(), 'gpt2_8bit_quantized.pth')
        return self.model_8bit

    def load_4bit_model(self):
        """Load 4-bit quantized model"""
        self.model_4bit = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            device_map='auto'
        )
        torch.save(self.model_4bit.state_dict(), 'gpt2_4bit_quantized.pth')
        return self.model_4bit

    def load_nf4_model(self):
        """Load NF4 quantized model"""
        self.model_nf4 = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_quant_type="nf4",
            device_map='auto'
        )
        torch.save(self.model_nf4.state_dict(), 'gpt2_nf4_quantized.pth')
        return self.model_nf4

"""## Prepare the Dataset"""

def prepare_dataset(tokenizer, max_length=512):
    """Prepare Wikipedia dataset for evaluation"""
    dataset = load_dataset("wikipedia", "20220301.en", split="train[:3000]")

    def tokenize_function(examples):
        return tokenizer(
            examples['text'],
            padding='max_length',
            truncation=True,
            max_length=max_length,
            return_tensors='pt'
        )

    encodings = tokenize_function(dataset)
    dataset = TextDataset(encodings)
    dataloader = DataLoader(dataset, batch_size=8, shuffle=False)

    return dataloader

"""## Functions to Calculate Perplexity and Latency"""

def calculate_perplexity(model, eval_dataloader, device):
    """Calculate model perplexity"""
    model.eval()
    total_loss = 0
    total_length = 0

    with torch.no_grad():
        for batch in tqdm(eval_dataloader, desc="Calculating perplexity"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = input_ids.clone()

            outputs = model(input_ids, attention_mask=attention_mask)
            logits = outputs.logits

            loss_fct = torch.nn.CrossEntropyLoss(reduction='sum')
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)),
                          shift_labels.view(-1))

            total_loss += loss.item()
            total_length += attention_mask.sum().item()

    return torch.exp(torch.tensor(total_loss / total_length))

def measure_inference_latency(model, input_ids, attention_mask, num_runs=200):
    """Measure inference latency"""
    latencies = []
    model.eval()

    with torch.no_grad():
        # Warmup
        for _ in range(10):
            _ = model(input_ids, attention_mask=attention_mask)

        # Measure latency
        for _ in tqdm(range(num_runs), desc="Measuring latency"):
            start_time = time.time()
            _ = model(input_ids, attention_mask=attention_mask)
            latencies.append((time.time() - start_time) * 1000)  # Convert to ms

    return np.mean(latencies), np.std(latencies)

def plot_comparison(metrics, title, ylabel):
    """Plot comparison metrics with enhanced y-axis scaling"""
    plt.figure(figsize=(6, 6))
    x = range(len(metrics['labels']))

    # Create bar plot
    bars = plt.bar(x, metrics['values'], color='skyblue')
    plt.xticks(x, metrics['labels'], rotation=45)
    plt.title(title)
    plt.ylabel(ylabel)

    # Add value labels on top of bars
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width() / 2., height,
                 f'{height:.2f}',
                 ha='center', va='bottom')

    # Adjust y-axis scale for better visualization
    y_min = min(metrics['values']) * 0.95
    y_max = max(metrics['values']) * 1.05
    plt.ylim(y_min, y_max)

    # Add grid lines for better readability
    plt.grid(axis='y', linestyle='--', alpha=0.5)

    plt.tight_layout()
    plt.show()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Initialize components
tokenizer = AutoTokenizer.from_pretrained('gpt2')
tokenizer.pad_token = tokenizer.eos_token
quantizer = ModelQuantizer('gpt2', device)

# Prepare dataset
eval_dataloader = prepare_dataset(tokenizer)

# Dictionary to store results
results = {
    'memory': {'labels': [], 'values': []},
    'perplexity': {'labels': [], 'values': []},
    'latency': {'labels': [], 'values': []},
}

# Test original model
print("\nTesting original model...")
model = quantizer.load_original_model()
sample_batch = next(iter(eval_dataloader))

results['memory']['labels'].append('Original')
results['memory']['values'].append(calculate_model_memory(model))


perplexity = calculate_perplexity(model, eval_dataloader, device)
results['perplexity']['labels'].append('Original')
results['perplexity']['values'].append(perplexity.item())

latency, _ = measure_inference_latency(
    model,
    sample_batch['input_ids'].to(device),
    sample_batch['attention_mask'].to(device)
)
results['latency']['labels'].append('Original')
results['latency']['values'].append(latency)

del model
gc.collect()
torch.cuda.empty_cache()

# Test 8-bit quantization
print("\nTesting 8-bit quantization...")
model = quantizer.load_8bit_model()

results['memory']['labels'].append('8-bit')
results['memory']['values'].append(calculate_model_memory(model,8))

perplexity = calculate_perplexity(model, eval_dataloader, device)
results['perplexity']['labels'].append('8-bit')
results['perplexity']['values'].append(perplexity.item())

latency, _ = measure_inference_latency(
    model,
    sample_batch['input_ids'].to(device),
    sample_batch['attention_mask'].to(device)
)

results['latency']['labels'].append('8-bit')
results['latency']['values'].append(latency)

del model
gc.collect()
torch.cuda.empty_cache()

# Test 4-bit linear quantization
print("\nTesting 4-bit linear quantization...")
model = quantizer.load_4bit_model()


# Track memory usage
results['memory']['labels'].append('4-bit')
results['memory']['values'].append(calculate_model_memory(model,4))

perplexity = calculate_perplexity(model, eval_dataloader, device)
results['perplexity']['labels'].append('4-bit')
results['perplexity']['values'].append(perplexity.item())

latency, _ = measure_inference_latency(
    model,
    sample_batch['input_ids'].to(device),
    sample_batch['attention_mask'].to(device)
)
results['latency']['labels'].append('4-bit')
results['latency']['values'].append(latency)

del model
gc.collect()
torch.cuda.empty_cache()

# Test NF4 quantization
print("\nTesting NF4 quantization...")
model = quantizer.load_nf4_model()

results['memory']['labels'].append('NF4')
results['memory']['values'].append(calculate_model_memory(model,4))

perplexity = calculate_perplexity(model, eval_dataloader, device)
results['perplexity']['labels'].append('NF4')
results['perplexity']['values'].append(perplexity.item())

latency, _ = measure_inference_latency(
    model,
    sample_batch['input_ids'].to(device),
    sample_batch['attention_mask'].to(device)
)
results['latency']['labels'].append('NF4')
results['latency']['values'].append(latency)

# Print summary
print("\nQuantization Results Summary:")
print("\nMemory Usage (MB):")
for label, value in zip(results['memory']['labels'], results['memory']['values']):
    print(f"  {label}: {value:.2f}")

print("\nPerplexity:")
for label, value in zip(results['perplexity']['labels'], results['perplexity']['values']):
    print(f"  {label}: {value:.2f}")

print("\nInference Latency (ms):")
for label, value in zip(results['latency']['labels'], results['latency']['values']):
    print(f"  {label}: {value:.2f}")

# Plot results directly without saving to a file
plot_comparison(results['memory'],
               'Memory Usage Comparison',
               'Memory (MB)')

plot_comparison(results['perplexity'],
               'Perplexity Comparison',
               'Perplexity')

plot_comparison(results['latency'],
               'Inference Latency Comparison',
               'Latency (ms)')

