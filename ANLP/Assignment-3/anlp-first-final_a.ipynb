{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2734496,"sourceType":"datasetVersion","datasetId":1654566}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-10-28T06:24:40.575544Z","iopub.execute_input":"2024-10-28T06:24:40.576204Z","iopub.status.idle":"2024-10-28T06:24:55.269847Z","shell.execute_reply.started":"2024-10-28T06:24:40.576162Z","shell.execute_reply":"2024-10-28T06:24:55.268805Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=a32336caee860aee33fb20d9449058751516567c774709d7f6a602b37aa82cf5\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\nimport torch\nfrom transformers import (\n    GPT2Tokenizer,\n    GPT2LMHeadModel,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForSeq2Seq,\n    AutoConfig\n)\nimport time\nimport re\nfrom rouge_score import rouge_scorer\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-28T06:24:55.272230Z","iopub.execute_input":"2024-10-28T06:24:55.273243Z","iopub.status.idle":"2024-10-28T06:24:56.108708Z","shell.execute_reply.started":"2024-10-28T06:24:55.273184Z","shell.execute_reply":"2024-10-28T06:24:56.107741Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\n# Function to clean and preprocess text\ndef clean_text(text):\n    # Lowercase\n    text = text.lower()\n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    # Remove special characters and digits\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    return text.strip()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-28T06:25:07.665879Z","iopub.execute_input":"2024-10-28T06:25:07.666848Z","iopub.status.idle":"2024-10-28T06:25:07.673689Z","shell.execute_reply.started":"2024-10-28T06:25:07.666792Z","shell.execute_reply":"2024-10-28T06:25:07.672762Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Load dataset\ndataset = load_dataset(\"/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail\", 'default')\ndataset = dataset.shuffle(seed=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-28T06:25:52.072536Z","iopub.execute_input":"2024-10-28T06:25:52.072945Z","iopub.status.idle":"2024-10-28T06:26:24.920092Z","shell.execute_reply.started":"2024-10-28T06:25:52.072903Z","shell.execute_reply":"2024-10-28T06:26:24.918911Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e1b5a2f97d24434bbabb125a67810e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d44a94b7d1154d908ebc747b9ec49612"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"953a207910a74b83b4dae00eaf73936f"}},"metadata":{}}]},{"cell_type":"code","source":"\n# Split into train, validation, and test sets\ntrain_size, val_size, test_size = 21000, 6000, 3000\ntrain_dataset = dataset['train'].select(range(train_size))\nval_dataset = dataset['train'].select(range(train_size, train_size + val_size))\ntest_dataset = dataset['train'].select(range(train_size + val_size, train_size + val_size + test_size))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-28T07:11:28.995015Z","iopub.execute_input":"2024-10-28T07:11:28.995724Z","iopub.status.idle":"2024-10-28T07:11:29.020372Z","shell.execute_reply.started":"2024-10-28T07:11:28.995675Z","shell.execute_reply":"2024-10-28T07:11:29.019587Z"},"trusted":true},"execution_count":150,"outputs":[]},{"cell_type":"code","source":"# Print sizes of the splits to verify\nprint(f\"Train set size: {len(train_dataset)}\")\nprint(f\"Validation set size: {len(val_dataset)}\")\nprint(f\"Test set size: {len(test_dataset)}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-28T07:11:29.484418Z","iopub.execute_input":"2024-10-28T07:11:29.485206Z","iopub.status.idle":"2024-10-28T07:11:29.492669Z","shell.execute_reply.started":"2024-10-28T07:11:29.485166Z","shell.execute_reply":"2024-10-28T07:11:29.491708Z"},"trusted":true},"execution_count":151,"outputs":[{"name":"stdout","text":"Train set size: 21000\nValidation set size: 6000\nTest set size: 3000\n","output_type":"stream"}]},{"cell_type":"code","source":"# Prepare and clean stories and summaries\nstories = [clean_text(item['article']) for item in train_dataset]\nsummaries = [clean_text(item['highlights']) for item in train_dataset]\n\n# Limit story length to be greater than summary length and apply maximum length constraint\nfinal_stories, final_summaries = [], []\nfor story, summary in zip(stories, summaries):\n    if len(story.split()) >= len(summary.split()) and len(story.split()) < 495:\n        final_stories.append(story)\n        final_summaries.append(summary)\n    if len(final_stories) > 35000:\n        break\n\nprint(f\"Final train stories count: {len(final_stories)}\")\nprint(f\"Final train summaries count: {len(final_summaries)}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-28T07:11:29.575493Z","iopub.execute_input":"2024-10-28T07:11:29.576244Z","iopub.status.idle":"2024-10-28T07:11:42.164653Z","shell.execute_reply.started":"2024-10-28T07:11:29.576196Z","shell.execute_reply":"2024-10-28T07:11:42.163689Z"},"trusted":true},"execution_count":152,"outputs":[{"name":"stdout","text":"Final train stories count: 7131\nFinal train summaries count: 7131\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import GPT2Tokenizer, AutoConfig, GPT2LMHeadModel\nfrom torch import nn\nimport torch.nn.functional as F\n\n# Initialize the tokenizer and model, freeze model parameters\nmodel_name = \"gpt2\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\nconfig = AutoConfig.from_pretrained(model_name)\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\n\n# Freeze the GPT-2 model parameters\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Define the Soft Prompt Embedding layer\nclass SoftPromptModel(nn.Module):\n    def __init__(self, model, num_prompts, embedding_size):\n        super(SoftPromptModel, self).__init__()\n        self.model = model\n        self.num_prompts = num_prompts\n        self.embedding_size = embedding_size\n\n        # Define the soft prompt embedding layer with dimensions [num_prompts, embedding_size]\n        self.soft_prompt_embeddings = nn.Embedding(num_prompts, embedding_size)\n\n    def forward(self, input_ids, attention_mask=None, labels=None):\n        # Get the soft prompt embeddings and expand to match the batch size\n        batch_size = input_ids.size(0)\n        soft_prompt_ids = torch.arange(self.num_prompts).unsqueeze(0).to(input_ids.device)\n        soft_prompt_embeddings = self.soft_prompt_embeddings(soft_prompt_ids)\n        soft_prompt_embeddings = soft_prompt_embeddings.expand(batch_size, -1, -1)\n\n        # Get the token embeddings from the model's input embeddings\n        input_embeddings = self.model.transformer.wte(input_ids)\n        \n        # Prepend the soft prompt embeddings to the input embeddings\n        inputs_embeds = torch.cat([soft_prompt_embeddings, input_embeddings], dim=1)\n\n        # Extend attention mask to cover soft prompts\n        if attention_mask is not None:\n            soft_prompt_attention_mask = torch.ones((batch_size, self.num_prompts), device=input_ids.device)\n            attention_mask = torch.cat([soft_prompt_attention_mask, attention_mask], dim=1)\n\n        # Forward pass through GPT-2 with modified inputs_embeds and attention_mask\n        output = self.model(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n        \n        # Compute the loss if labels are provided\n        loss = None\n        if labels is not None:\n            # Shift labels and input embeddings to match dimensions for loss calculation\n            shift_logits = output.logits[..., self.num_prompts:-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n\n            # Compute loss using CrossEntropyLoss, ignoring padding tokens\n            loss_fct = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n        \n        return {\"loss\": loss, \"logits\": output.logits}\n\n# Set hyperparameters for the soft prompt\nnum_prompts = 3  # Define the number of prompt tokens\nembedding_size = model.config.hidden_size  # 768 for GPT-2 small\n\n# Initialize the soft prompt model with the frozen GPT-2 model and soft prompt embeddings\nsoft_prompt_model = SoftPromptModel(model, num_prompts, embedding_size)","metadata":{"execution":{"iopub.status.busy":"2024-10-28T07:11:42.166395Z","iopub.execute_input":"2024-10-28T07:11:42.166704Z","iopub.status.idle":"2024-10-28T07:11:42.903289Z","shell.execute_reply.started":"2024-10-28T07:11:42.166671Z","shell.execute_reply":"2024-10-28T07:11:42.902442Z"},"trusted":true},"execution_count":153,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Initialize the soft prompt model with the frozen GPT-2 model and soft prompt embeddings\nmodel = SoftPromptModel(model, num_prompts, embedding_size)","metadata":{"execution":{"iopub.status.busy":"2024-10-28T07:11:42.904707Z","iopub.execute_input":"2024-10-28T07:11:42.905181Z","iopub.status.idle":"2024-10-28T07:11:42.911290Z","shell.execute_reply.started":"2024-10-28T07:11:42.905135Z","shell.execute_reply":"2024-10-28T07:11:42.910328Z"},"trusted":true},"execution_count":154,"outputs":[]},{"cell_type":"code","source":"# Print trainable parameters to verify\nfor name, param in model.named_parameters():\n    if param.requires_grad:\n        print(f\"{name} is trainable\")\n    else:\n        print(f\"{name} is frozen\")","metadata":{"execution":{"iopub.status.busy":"2024-10-28T07:11:42.913509Z","iopub.execute_input":"2024-10-28T07:11:42.913866Z","iopub.status.idle":"2024-10-28T07:11:42.951194Z","shell.execute_reply.started":"2024-10-28T07:11:42.913831Z","shell.execute_reply":"2024-10-28T07:11:42.950180Z"},"trusted":true},"execution_count":155,"outputs":[{"name":"stdout","text":"model.transformer.wte.weight is frozen\nmodel.transformer.wpe.weight is frozen\nmodel.transformer.h.0.ln_1.weight is frozen\nmodel.transformer.h.0.ln_1.bias is frozen\nmodel.transformer.h.0.attn.c_attn.weight is frozen\nmodel.transformer.h.0.attn.c_attn.bias is frozen\nmodel.transformer.h.0.attn.c_proj.weight is frozen\nmodel.transformer.h.0.attn.c_proj.bias is frozen\nmodel.transformer.h.0.ln_2.weight is frozen\nmodel.transformer.h.0.ln_2.bias is frozen\nmodel.transformer.h.0.mlp.c_fc.weight is frozen\nmodel.transformer.h.0.mlp.c_fc.bias is frozen\nmodel.transformer.h.0.mlp.c_proj.weight is frozen\nmodel.transformer.h.0.mlp.c_proj.bias is frozen\nmodel.transformer.h.1.ln_1.weight is frozen\nmodel.transformer.h.1.ln_1.bias is frozen\nmodel.transformer.h.1.attn.c_attn.weight is frozen\nmodel.transformer.h.1.attn.c_attn.bias is frozen\nmodel.transformer.h.1.attn.c_proj.weight is frozen\nmodel.transformer.h.1.attn.c_proj.bias is frozen\nmodel.transformer.h.1.ln_2.weight is frozen\nmodel.transformer.h.1.ln_2.bias is frozen\nmodel.transformer.h.1.mlp.c_fc.weight is frozen\nmodel.transformer.h.1.mlp.c_fc.bias is frozen\nmodel.transformer.h.1.mlp.c_proj.weight is frozen\nmodel.transformer.h.1.mlp.c_proj.bias is frozen\nmodel.transformer.h.2.ln_1.weight is frozen\nmodel.transformer.h.2.ln_1.bias is frozen\nmodel.transformer.h.2.attn.c_attn.weight is frozen\nmodel.transformer.h.2.attn.c_attn.bias is frozen\nmodel.transformer.h.2.attn.c_proj.weight is frozen\nmodel.transformer.h.2.attn.c_proj.bias is frozen\nmodel.transformer.h.2.ln_2.weight is frozen\nmodel.transformer.h.2.ln_2.bias is frozen\nmodel.transformer.h.2.mlp.c_fc.weight is frozen\nmodel.transformer.h.2.mlp.c_fc.bias is frozen\nmodel.transformer.h.2.mlp.c_proj.weight is frozen\nmodel.transformer.h.2.mlp.c_proj.bias is frozen\nmodel.transformer.h.3.ln_1.weight is frozen\nmodel.transformer.h.3.ln_1.bias is frozen\nmodel.transformer.h.3.attn.c_attn.weight is frozen\nmodel.transformer.h.3.attn.c_attn.bias is frozen\nmodel.transformer.h.3.attn.c_proj.weight is frozen\nmodel.transformer.h.3.attn.c_proj.bias is frozen\nmodel.transformer.h.3.ln_2.weight is frozen\nmodel.transformer.h.3.ln_2.bias is frozen\nmodel.transformer.h.3.mlp.c_fc.weight is frozen\nmodel.transformer.h.3.mlp.c_fc.bias is frozen\nmodel.transformer.h.3.mlp.c_proj.weight is frozen\nmodel.transformer.h.3.mlp.c_proj.bias is frozen\nmodel.transformer.h.4.ln_1.weight is frozen\nmodel.transformer.h.4.ln_1.bias is frozen\nmodel.transformer.h.4.attn.c_attn.weight is frozen\nmodel.transformer.h.4.attn.c_attn.bias is frozen\nmodel.transformer.h.4.attn.c_proj.weight is frozen\nmodel.transformer.h.4.attn.c_proj.bias is frozen\nmodel.transformer.h.4.ln_2.weight is frozen\nmodel.transformer.h.4.ln_2.bias is frozen\nmodel.transformer.h.4.mlp.c_fc.weight is frozen\nmodel.transformer.h.4.mlp.c_fc.bias is frozen\nmodel.transformer.h.4.mlp.c_proj.weight is frozen\nmodel.transformer.h.4.mlp.c_proj.bias is frozen\nmodel.transformer.h.5.ln_1.weight is frozen\nmodel.transformer.h.5.ln_1.bias is frozen\nmodel.transformer.h.5.attn.c_attn.weight is frozen\nmodel.transformer.h.5.attn.c_attn.bias is frozen\nmodel.transformer.h.5.attn.c_proj.weight is frozen\nmodel.transformer.h.5.attn.c_proj.bias is frozen\nmodel.transformer.h.5.ln_2.weight is frozen\nmodel.transformer.h.5.ln_2.bias is frozen\nmodel.transformer.h.5.mlp.c_fc.weight is frozen\nmodel.transformer.h.5.mlp.c_fc.bias is frozen\nmodel.transformer.h.5.mlp.c_proj.weight is frozen\nmodel.transformer.h.5.mlp.c_proj.bias is frozen\nmodel.transformer.h.6.ln_1.weight is frozen\nmodel.transformer.h.6.ln_1.bias is frozen\nmodel.transformer.h.6.attn.c_attn.weight is frozen\nmodel.transformer.h.6.attn.c_attn.bias is frozen\nmodel.transformer.h.6.attn.c_proj.weight is frozen\nmodel.transformer.h.6.attn.c_proj.bias is frozen\nmodel.transformer.h.6.ln_2.weight is frozen\nmodel.transformer.h.6.ln_2.bias is frozen\nmodel.transformer.h.6.mlp.c_fc.weight is frozen\nmodel.transformer.h.6.mlp.c_fc.bias is frozen\nmodel.transformer.h.6.mlp.c_proj.weight is frozen\nmodel.transformer.h.6.mlp.c_proj.bias is frozen\nmodel.transformer.h.7.ln_1.weight is frozen\nmodel.transformer.h.7.ln_1.bias is frozen\nmodel.transformer.h.7.attn.c_attn.weight is frozen\nmodel.transformer.h.7.attn.c_attn.bias is frozen\nmodel.transformer.h.7.attn.c_proj.weight is frozen\nmodel.transformer.h.7.attn.c_proj.bias is frozen\nmodel.transformer.h.7.ln_2.weight is frozen\nmodel.transformer.h.7.ln_2.bias is frozen\nmodel.transformer.h.7.mlp.c_fc.weight is frozen\nmodel.transformer.h.7.mlp.c_fc.bias is frozen\nmodel.transformer.h.7.mlp.c_proj.weight is frozen\nmodel.transformer.h.7.mlp.c_proj.bias is frozen\nmodel.transformer.h.8.ln_1.weight is frozen\nmodel.transformer.h.8.ln_1.bias is frozen\nmodel.transformer.h.8.attn.c_attn.weight is frozen\nmodel.transformer.h.8.attn.c_attn.bias is frozen\nmodel.transformer.h.8.attn.c_proj.weight is frozen\nmodel.transformer.h.8.attn.c_proj.bias is frozen\nmodel.transformer.h.8.ln_2.weight is frozen\nmodel.transformer.h.8.ln_2.bias is frozen\nmodel.transformer.h.8.mlp.c_fc.weight is frozen\nmodel.transformer.h.8.mlp.c_fc.bias is frozen\nmodel.transformer.h.8.mlp.c_proj.weight is frozen\nmodel.transformer.h.8.mlp.c_proj.bias is frozen\nmodel.transformer.h.9.ln_1.weight is frozen\nmodel.transformer.h.9.ln_1.bias is frozen\nmodel.transformer.h.9.attn.c_attn.weight is frozen\nmodel.transformer.h.9.attn.c_attn.bias is frozen\nmodel.transformer.h.9.attn.c_proj.weight is frozen\nmodel.transformer.h.9.attn.c_proj.bias is frozen\nmodel.transformer.h.9.ln_2.weight is frozen\nmodel.transformer.h.9.ln_2.bias is frozen\nmodel.transformer.h.9.mlp.c_fc.weight is frozen\nmodel.transformer.h.9.mlp.c_fc.bias is frozen\nmodel.transformer.h.9.mlp.c_proj.weight is frozen\nmodel.transformer.h.9.mlp.c_proj.bias is frozen\nmodel.transformer.h.10.ln_1.weight is frozen\nmodel.transformer.h.10.ln_1.bias is frozen\nmodel.transformer.h.10.attn.c_attn.weight is frozen\nmodel.transformer.h.10.attn.c_attn.bias is frozen\nmodel.transformer.h.10.attn.c_proj.weight is frozen\nmodel.transformer.h.10.attn.c_proj.bias is frozen\nmodel.transformer.h.10.ln_2.weight is frozen\nmodel.transformer.h.10.ln_2.bias is frozen\nmodel.transformer.h.10.mlp.c_fc.weight is frozen\nmodel.transformer.h.10.mlp.c_fc.bias is frozen\nmodel.transformer.h.10.mlp.c_proj.weight is frozen\nmodel.transformer.h.10.mlp.c_proj.bias is frozen\nmodel.transformer.h.11.ln_1.weight is frozen\nmodel.transformer.h.11.ln_1.bias is frozen\nmodel.transformer.h.11.attn.c_attn.weight is frozen\nmodel.transformer.h.11.attn.c_attn.bias is frozen\nmodel.transformer.h.11.attn.c_proj.weight is frozen\nmodel.transformer.h.11.attn.c_proj.bias is frozen\nmodel.transformer.h.11.ln_2.weight is frozen\nmodel.transformer.h.11.ln_2.bias is frozen\nmodel.transformer.h.11.mlp.c_fc.weight is frozen\nmodel.transformer.h.11.mlp.c_fc.bias is frozen\nmodel.transformer.h.11.mlp.c_proj.weight is frozen\nmodel.transformer.h.11.mlp.c_proj.bias is frozen\nmodel.transformer.ln_f.weight is frozen\nmodel.transformer.ln_f.bias is frozen\nsoft_prompt_embeddings.weight is trainable\n","output_type":"stream"}]},{"cell_type":"code","source":"# Count trainable and frozen parameters\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nfrozen_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n\nprint(f\"Trainable parameters: {trainable_params}\")\nprint(f\"Frozen parameters: {frozen_params}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-28T07:11:42.952903Z","iopub.execute_input":"2024-10-28T07:11:42.953304Z","iopub.status.idle":"2024-10-28T07:11:42.963158Z","shell.execute_reply.started":"2024-10-28T07:11:42.953257Z","shell.execute_reply":"2024-10-28T07:11:42.962202Z"},"trusted":true},"execution_count":156,"outputs":[{"name":"stdout","text":"Trainable parameters: 2304\nFrozen parameters: 124439808\n","output_type":"stream"}]},{"cell_type":"code","source":"# Dataset class for PyTorch\nclass SummDataset(torch.utils.data.Dataset):\n    def __init__(self, stories_list, summaries_list):\n        self.stor_list = stories_list\n        self.sum_list = summaries_list\n        self.max_length = 500\n\n    def encode(self, text):\n        return tokenizer.encode(text, return_tensors=\"pt\", max_length=self.max_length, truncation=True, padding=\"max_length\")\n\n    def __len__(self):\n        return len(self.stor_list)\n\n    def __getitem__(self, index):\n        story = self.encode(self.stor_list[index]).squeeze(0)\n        summary = self.encode(self.sum_list[index]).squeeze(0)\n        return {\n            'input_ids': story,\n            'labels': summary\n        }\n","metadata":{"execution":{"iopub.status.busy":"2024-10-28T07:11:42.964807Z","iopub.execute_input":"2024-10-28T07:11:42.965451Z","iopub.status.idle":"2024-10-28T07:11:42.974762Z","shell.execute_reply.started":"2024-10-28T07:11:42.965407Z","shell.execute_reply":"2024-10-28T07:11:42.973997Z"},"trusted":true},"execution_count":157,"outputs":[]},{"cell_type":"code","source":"print(val_dataset[0])","metadata":{"execution":{"iopub.status.busy":"2024-10-28T07:11:42.975890Z","iopub.execute_input":"2024-10-28T07:11:42.976193Z","iopub.status.idle":"2024-10-28T07:11:42.987274Z","shell.execute_reply.started":"2024-10-28T07:11:42.976160Z","shell.execute_reply":"2024-10-28T07:11:42.985985Z"},"trusted":true},"execution_count":158,"outputs":[{"name":"stdout","text":"{'id': 'f12b2293dd1351160f08b5484f0d5ca2d6aa1aef', 'article': \"CLICK HERE to read Matt Lawton assessment of Roy Keane's explosive press conference . Gordon Strachan will exercise his right of reply to Roy Keane's book comments after Scotland's 2016 European Championship qualifying double-header against Georgia and Poland. In his explosive new autobiography, the Irishman said defiance underpinned his move to Celtic in January 2006, as he claimed that then Hoops boss Strachan was less than enthusiastic. VIDEO Scroll down to see what happened when a phone interrupted Keane's book launch . Gordon Strachan (L) and Roy Keane pose with the shirt after midfielder signed for Celtic in 2006 . Keane has claimed in his autobiography that he only signed for the Hoops to defy manager Strachan . Keane retired from playing six months after joining Celtic and collecting a domestic double . Keane said: 'I met Gordon Strachan in London at Dermot Desmond's (Celtic majority shareholder) house. 'Gordon told me 'I'm not really worried if you sign for us or not. We're okay without you'. 'He was letting me know he wasn't desperate for me; he was being a bit coy. But there was a bit of defiance from me.' However, at his media conference ahead of the Group D game against Georgia at Ibrox on Saturday, Strachan laughed when asked what he thought about the Aston Villa assistant manager's comments. He said: 'I think he is fantastic. I get on great with him. Keane sits at the Aviva stadium in Dublin for the launch of his autobiography, The Second Half . Despite claims from the book, Strachan insists he has a 'smashing' relationship with Keane . 'I have no idea what he said in his book, I am sure I will have a look but we get on smashing together. 'But this is not the right time to talk about his book. Come and see me in two weeks' time and we can talk about it when I have had time to read it. 'But I am sure it is interesting reading. 'It is always good fun and also it is a bit tongue in cheek with Roy at times.'\", 'highlights': \"Roy Keane's latest autobiography, The Second Half, has caused controversy .\\nFormer Manchester United star signed for Gordon Strachan's Celtic in 2006 .\\nAston Villa assistant claims he only joined to defy the now Scotland boss .\"}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create dataset instances\ntrain_dataset = SummDataset(final_stories, final_summaries)\n# val_stories = [clean_text(item['article']) for item in val_dataset]\n# val_summaries = [clean_text(item['highlights']) for item in val_dataset]\nval_stories = [clean_text(item['article']) for item in val_dataset][:200]\nval_summaries = [clean_text(item['highlights']) for item in val_dataset][:200]\n\nval_dataset=SummDataset(val_stories, val_summaries)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-28T07:11:42.989997Z","iopub.execute_input":"2024-10-28T07:11:42.990608Z","iopub.status.idle":"2024-10-28T07:11:46.091444Z","shell.execute_reply.started":"2024-10-28T07:11:42.990561Z","shell.execute_reply":"2024-10-28T07:11:46.090565Z"},"trusted":true},"execution_count":159,"outputs":[]},{"cell_type":"code","source":"test_stories = [clean_text(item['article']) for item in test_dataset]\ntest_summaries = [clean_text(item['highlights']) for item in test_dataset]\n\ntest_dataset=SummDataset(test_stories, test_summaries)\n# Data collator\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)","metadata":{"execution":{"iopub.status.busy":"2024-10-28T07:11:46.092523Z","iopub.execute_input":"2024-10-28T07:11:46.092813Z","iopub.status.idle":"2024-10-28T07:11:47.652722Z","shell.execute_reply.started":"2024-10-28T07:11:46.092781Z","shell.execute_reply":"2024-10-28T07:11:47.651803Z"},"trusted":true},"execution_count":160,"outputs":[]},{"cell_type":"code","source":"\n# Define the compute_metrics function for ROUGE score calculation\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Calculate ROUGE scores\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    scores = {\n        'rouge-1': 0.0,\n        'rouge-2': 0.0,\n        'rouge-l': 0.0,\n    }\n    for pred, label in zip(decoded_preds, decoded_labels):\n        score = scorer.score(label, pred)\n        scores['rouge-1'] += score['rouge1'].fmeasure\n        scores['rouge-2'] += score['rouge2'].fmeasure\n        scores['rouge-l'] += score['rougeL'].fmeasure\n\n    # Average scores\n    total = len(decoded_preds)\n    scores = {k: v / total for k, v in scores.items()}\n    return scores\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-28T08:06:20.170876Z","iopub.execute_input":"2024-10-28T08:06:20.171533Z","iopub.status.idle":"2024-10-28T08:06:20.180720Z","shell.execute_reply.started":"2024-10-28T08:06:20.171492Z","shell.execute_reply":"2024-10-28T08:06:20.179789Z"},"trusted":true},"execution_count":200,"outputs":[]},{"cell_type":"code","source":"!rm -rf /kaggle/working/*","metadata":{"execution":{"iopub.status.busy":"2024-10-28T07:16:05.849544Z","iopub.execute_input":"2024-10-28T07:16:05.850287Z","iopub.status.idle":"2024-10-28T07:16:06.966689Z","shell.execute_reply.started":"2024-10-28T07:16:05.850246Z","shell.execute_reply":"2024-10-28T07:16:06.965752Z"},"trusted":true},"execution_count":173,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, EarlyStoppingCallback\n","metadata":{"execution":{"iopub.status.busy":"2024-10-28T07:16:06.969145Z","iopub.execute_input":"2024-10-28T07:16:06.969682Z","iopub.status.idle":"2024-10-28T07:16:06.975674Z","shell.execute_reply.started":"2024-10-28T07:16:06.969627Z","shell.execute_reply":"2024-10-28T07:16:06.974895Z"},"trusted":true},"execution_count":174,"outputs":[]},{"cell_type":"code","source":"\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"Final_results\",\n    evaluation_strategy=\"steps\",\n    eval_steps=100,\n    logging_steps=100,\n    save_steps=100,\n    learning_rate=3e-4,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=2,\n    num_train_epochs=10,\n    weight_decay=0.01,\n    save_total_limit=2,\n    report_to=\"none\",\n    no_cuda=False,\n    load_best_model_at_end=True,\n    metric_for_best_model='eval_loss',\n    greater_is_better=False,\n    save_safetensors=False\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-28T07:16:18.893229Z","iopub.execute_input":"2024-10-28T07:16:18.894018Z","iopub.status.idle":"2024-10-28T07:16:18.925960Z","shell.execute_reply.started":"2024-10-28T07:16:18.893975Z","shell.execute_reply":"2024-10-28T07:16:18.924999Z"},"trusted":true},"execution_count":175,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Trainer initialization\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-28T07:16:19.655842Z","iopub.execute_input":"2024-10-28T07:16:19.656230Z","iopub.status.idle":"2024-10-28T07:16:19.675686Z","shell.execute_reply.started":"2024-10-28T07:16:19.656194Z","shell.execute_reply":"2024-10-28T07:16:19.674893Z"},"trusted":true},"execution_count":176,"outputs":[]},{"cell_type":"code","source":"# Training\nstart_time = time.time()\ntrainer.train()\ntrainer.save_model(training_args.output_dir)\ntraining_time = time.time() - start_time","metadata":{"execution":{"iopub.status.busy":"2024-10-28T07:16:20.362899Z","iopub.execute_input":"2024-10-28T07:16:20.363291Z","iopub.status.idle":"2024-10-28T07:46:54.779841Z","shell.execute_reply.started":"2024-10-28T07:16:20.363251Z","shell.execute_reply":"2024-10-28T07:46:54.778788Z"},"trusted":true},"execution_count":177,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5349' max='5349' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5349/5349 30:32, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>9.733300</td>\n      <td>10.047009</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>9.641100</td>\n      <td>9.978683</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>9.508800</td>\n      <td>9.912032</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>9.446300</td>\n      <td>9.844051</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>9.489100</td>\n      <td>9.774864</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>9.292400</td>\n      <td>9.714208</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>9.287000</td>\n      <td>9.660138</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>9.303300</td>\n      <td>9.617360</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>9.237100</td>\n      <td>9.581841</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>9.164700</td>\n      <td>9.537551</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>9.121800</td>\n      <td>9.489642</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>9.214600</td>\n      <td>9.447174</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>9.115000</td>\n      <td>9.413887</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>9.067400</td>\n      <td>9.367474</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>9.072600</td>\n      <td>9.331943</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>9.031000</td>\n      <td>9.305124</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>9.005600</td>\n      <td>9.278263</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>8.962000</td>\n      <td>9.249636</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>8.981400</td>\n      <td>9.230726</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>8.988400</td>\n      <td>9.253292</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>8.980500</td>\n      <td>9.243222</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>8.929900</td>\n      <td>9.190871</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>8.932900</td>\n      <td>9.194952</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>8.969700</td>\n      <td>9.195110</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>8.939200</td>\n      <td>9.181402</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>8.928700</td>\n      <td>9.166047</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>8.897300</td>\n      <td>9.143859</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>8.914800</td>\n      <td>9.133821</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>8.919800</td>\n      <td>9.124765</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>8.864100</td>\n      <td>9.113955</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>8.903300</td>\n      <td>9.099448</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>8.906300</td>\n      <td>9.082402</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>8.893900</td>\n      <td>9.074991</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>8.888200</td>\n      <td>9.064612</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>8.870400</td>\n      <td>9.044688</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>8.853400</td>\n      <td>9.037215</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>8.854900</td>\n      <td>9.031110</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>8.881300</td>\n      <td>8.992579</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>8.823400</td>\n      <td>8.981241</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>8.850700</td>\n      <td>8.975641</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>8.825000</td>\n      <td>8.970699</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>8.822200</td>\n      <td>8.964128</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>8.849000</td>\n      <td>8.956992</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>8.859100</td>\n      <td>8.952076</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>8.826000</td>\n      <td>8.949098</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>8.849200</td>\n      <td>8.945980</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>8.831600</td>\n      <td>8.943409</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>8.824000</td>\n      <td>8.940526</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>8.839700</td>\n      <td>8.938280</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>8.833900</td>\n      <td>8.937083</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>8.823600</td>\n      <td>8.935840</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>8.810900</td>\n      <td>8.935086</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>8.811000</td>\n      <td>8.934844</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"# Evaluate\nevaluation_results = trainer.evaluate()\nprint(f\"Evaluation results: {evaluation_results}\")\n\n# Number of parameters\nnum_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Number of trainable parameters: {num_params}\")\n\n# Estimate GPU compute (in FLOPs)\nflops = 2 * num_params * (training_args.per_device_train_batch_size * training_args.num_train_epochs)\nprint(f\"Estimated FLOPs: {flops}\")\n\n# GPU memory usage\ngpu_memory = torch.cuda.memory_allocated() / (1024**2)  # Convert to MB\nprint(f\"GPU memory allocated: {gpu_memory:.2f} MB\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-28T07:46:54.781777Z","iopub.execute_input":"2024-10-28T07:46:54.782108Z","iopub.status.idle":"2024-10-28T07:47:00.823628Z","shell.execute_reply.started":"2024-10-28T07:46:54.782073Z","shell.execute_reply":"2024-10-28T07:47:00.822671Z"},"trusted":true},"execution_count":178,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [25/25 17:03]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation results: {'eval_loss': 8.934844017028809, 'eval_runtime': 6.0305, 'eval_samples_per_second': 33.165, 'eval_steps_per_second': 4.146, 'epoch': 3.0}\nNumber of trainable parameters: 2304\nEstimated FLOPs: 27648\nGPU memory allocated: 1975.33 MB\n","output_type":"stream"}]},{"cell_type":"code","source":"# Trainer initialization\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-28T08:25:08.715022Z","iopub.execute_input":"2024-10-28T08:25:08.715904Z","iopub.status.idle":"2024-10-28T08:25:08.728396Z","shell.execute_reply.started":"2024-10-28T08:25:08.715864Z","shell.execute_reply":"2024-10-28T08:25:08.727404Z"},"trusted":true},"execution_count":227,"outputs":[]},{"cell_type":"code","source":"# Evaluate\nevaluation_results = test_trainer.evaluate()\nprint(f\"Evaluation results: {evaluation_results}\")\n","metadata":{},"execution_count":null,"outputs":[]}]}